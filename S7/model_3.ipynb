{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"icT72j1kizVg","executionInfo":{"status":"ok","timestamp":1710402636697,"user_tz":-60,"elapsed":5182,"user":{"displayName":"Arun Kutty","userId":"09223495942040897754"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"0gKZWya28NlN"},"source":["Target:\n","\n","    Add rotation of around 15 degrees for better variation.\n","    Reduce the channel size for reduction in parameters\n","    Reduced the dropout to a minimal value\n","    Results:\n","        Parameters: 7.4k\n","        Best Train Accuracy: 99.07\n","        Best Test Accuracy: 99.45 (10th epoch)\n","    Analysis:\n","        The model is underfitting now\n","        The test accuracy is also up, which means our test data had few images that had transformation difference w.r.t. train dataset\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"V8R9hzsLIoSi","executionInfo":{"status":"ok","timestamp":1710403258122,"user_tz":-60,"elapsed":248,"user":{"displayName":"Arun Kutty","userId":"09223495942040897754"}}},"outputs":[],"source":["class Net3(nn.Module):\n","    def __init__(self):\n","        super(Net3, self).__init__()\n","        drop = 0.025\n","\n","        self.convblock1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(8),\n","            nn.Dropout(drop)\n","        ) # output_size = 26    RF:  2\n","\n","        # CONVOLUTION BLOCK 1\n","        self.convblock2 = nn.Sequential(\n","            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16),\n","            nn.Dropout(drop)\n","        ) # output_size = 24 RF: 5\n","\n","\n","\n","        # TRANSITION BLOCK 1\n","        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12    RF:  6\n","        self.trans1 = nn.Sequential( #Antman\n","            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(8),\n","        ) # output_size = 12    RF:  6\n","\n","        # CONVOLUTION BLOCK 2\n","        self.convblock3 = nn.Sequential(\n","            nn.Conv2d(in_channels=8, out_channels=12, kernel_size=(3, 3), padding=0, bias=False), # output_size = 10    RF:  10\n","            nn.ReLU(),\n","            nn.BatchNorm2d(12),\n","            nn.Dropout(drop),\n","            nn.Conv2d(in_channels=12, out_channels=16, kernel_size=(3, 3), padding=0, bias=False), # output_size = 8    RF: 14\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16),\n","            nn.Dropout(drop),\n","            nn.Conv2d(in_channels=16, out_channels=20, kernel_size=(3, 3), padding=0, bias=False), # output_size = 6    RF: 18\n","            nn.ReLU(),\n","            nn.BatchNorm2d(20),\n","            nn.Dropout(drop)\n","        )\n","\n","        # Global average pooling\n","        self.gap = nn.Sequential(\n","            nn.AvgPool2d(6)\n","        ) # output_size = 1  RF:  28\n","\n","        #Optional Fully Connected Layer - Commenting out the fc1 and testing with 2 Conv layers as this gave a better result with less parameters\n","        self.convblock5 = nn.Sequential(\n","            nn.Conv2d(in_channels=20, out_channels=16, kernel_size=(1, 1), padding=0, bias=False), # output_size = 1    RF: 28\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16),\n","            nn.Dropout(drop),\n","            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),  # output  RF: 28\n","        )\n","        # self.fc1 = nn.Linear(20, 10)\n","\n","    def forward(self, x):\n","        x = self.convblock1(x)\n","        x = self.convblock2(x)\n","        x = self.pool1(x)\n","        x = self.trans1(x)\n","        x = self.convblock3(x)\n","        x = self.gap(x)\n","        x = self.convblock5(x)\n","        x = x.view(-1, 10)   # convert 2D to 1D\n","\n","        return F.log_softmax(x, dim=-1)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}